trainer:
  model_config:
    _target_: UnetModel
    network_shape:
      - [64, 3, 3, 2]
      - [128, 3, 3, 2]
      - [256, 3, 3, 2]
      - [512, 3, 2, 2]
    verbose: true
  train_dataset_config:
    _target_: KaeggleDataset
    dataset_path: null
    preload_all: false
    verbose: true
  val_dataset_config:
    _target_: KaeggleDataset
    dataset_path: null
    preload_all: false
    verbose: true
  test_dataset_config:
    _target_: TestDataset
    dataset_path: null
    preload_all: false
    test: true
    verbose: true
  dataloader_config:
    _target_: torch.utils.data.dataloader.DataLoader
    shuffle: true
    num_workers: 0
  optimizer_config:
    _target_: torch.optim.adam.Adam
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0
  scheduler_config:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    options:
      factor: 0.8
      patience: 5
      threshold: 0.001
      min_lr: 0.0001
  logging_config:
    checkpoint_path: checkpoints
  name: DefaultTrainer
  pretrained_path: null
  visualize_output: false
  use_cuda: false
  use_mps: false
  device_id: 0
  early_stoppage: false
  loss_threshold: 0.005
  batch_size: 8
  epochs: 101
  checkpoint_interval: 5
  half_precision: false
  clip_gradients: false
  torch_seed: 42
  use_dataset: true
  use_model: true
  wandb: false
  verbose: true
